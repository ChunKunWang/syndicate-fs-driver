#!/usr/bin/env python

"""
   Copyright 2016 The Trustees of Princeton University

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
"""

"""
Filesystem AG driver.
Serves files on a remote system through generic fs plugin.
"""

import traceback
import sys
import os
import errno
import time
import threading
import json
import syndicate.util.gateway as gateway
import sgfsdriver.lib.abstractfs as abstractfs

from sgfsdriver.lib.pluginloader import pluginloader

storage_dir = None
fs = None
lock = None

# will store events to be processed
event_queue = []

DEFAULT_WRITE_TTL = 1000 * 60 * 5 # 5mins

"""
AG update event
"""
class ag_update_event(object):
    ENTRY_UPDATED = 0
    ENTRY_ADDED = 1
    ENTRY_REMOVED = 2

    def __init__(self, flag=None,
                       path=None,
                       stat=None):
        self.flag = flag
        self.path = path
        self.stat = stat

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __repr__(self):
        rep_f = "UPDATE"
        if self.flag == self.ENTRY_UPDATED:
            rep_f = "UPDATE"
        elif self.flag == self.ENTRY_ADDED:
            rep_f = "ADD"
        elif self.flag == self.ENTRY_REMOVED:
            rep_f = "REMOVE"

        return "<ag_update_event %s %s %s>" % (rep_f, self.path, self.stat)

def _initFS( driver_config, driver_secrets, role ):
    global fs
    global storage_dir

    if fs:
        return True

    # continue only when fs is not initialized
    if not driver_config.has_key( 'DRIVER_FS_PLUGIN' ):
        gateway.log_error( "No DRIVER_FS_PLUGIN defined" )
        return False

    if not driver_config.has_key( 'DRIVER_FS_PLUGIN_CONFIG' ):
        gateway.log_error( "No DRIVER_FS_PLUGIN_CONFIG defined" )
        return False

    if not driver_config.has_key('DATASET_DIR'):
        gateway.log_error( "No DATASET_DIR defined" )
        return False

    dataset_dir = driver_config['DATASET_DIR']
    dataset_dir = "/" + dataset_dir.strip("/")

    plugin = driver_config['DRIVER_FS_PLUGIN']

    if isinstance( driver_config['DRIVER_FS_PLUGIN_CONFIG'], dict ):
        plugin_config = driver_config['DRIVER_FS_PLUGIN_CONFIG']
    elif isinstance( driver_config['DRIVER_FS_PLUGIN_CONFIG'], basestring ):
        json_plugin_config = driver_config['DRIVER_FS_PLUGIN_CONFIG']
        plugin_config = json.loads( json_plugin_config )

    plugin_config["secrets"] = driver_secrets
    plugin_config["work_root"] = dataset_dir

    try:
        loader = pluginloader()
        fs = loader.load( plugin, plugin_config, role )

        if not fs:
            gateway.log_error( "No such driver plugin found: %s" % plugin )
            return False

        fs.set_notification_cb(datasets_update_cb)
        fs.connect()
    except Exception as e:
        gateway.log_error( "Unable to initialize a driver" )
        gateway.log_error( str( e ) )
        traceback.print_exc()
        return False
    return True

def _shutdownFS():
    global fs
    if fs:
        try:
            fs.close()
        except Exception:
            pass
    fs = None

def datasets_update_cb( updated_entries, added_entries, removed_entries ):
    global event_queue
    global lock

    _lock()
    for u in updated_entries:
        entry = ag_update_event(ag_update_event.ENTRY_UPDATED, u.path, u.stat)
        event_queue.append(entry)

    for a in added_entries:
        entry = ag_update_event(ag_update_event.ENTRY_ADDED, a.path, a.stat)
        event_queue.append(entry)

    for r in removed_entries:
        entry = ag_update_event(ag_update_event.ENTRY_REMOVED, r.path, r.stat)
        event_queue.append(entry)
    _unlock()

def _lock():
    global lock
    lock.acquire()

def _unlock():
    global lock
    lock.release()

def _resync( path ):
    global fs

    stack = [path]
    while len( stack ) > 0:
        last_dir = stack.pop( 0 )
        fs.clear_cache( last_dir )
        entries = fs.list_dir( last_dir )
        for entry in entries:
            # entry is a filename
            entry_path = last_dir.rstrip( "/" ) + "/" + entry
            st = fs.stat( entry_path )
            e = abstractfs.afsevent( entry_path, st )

            if st.directory:
                stack.append( entry_path )

            datasets_update_cb( [], [e], [] )


def driver_init( driver_config, driver_secrets ):
    """
    Do the one-time driver setup.
    """

    global fs
    global storage_dir
    global lock

    # create a re-entrant lock (not a read lock)
    lock = threading.RLock()

    # detect a role
    rolestr = sys.argv[1]
    role = abstractfs.afsrole.DISCOVER
    if rolestr == "read":
        role = abstractfs.afsrole.READ
    elif rolestr == "crawl":
        role = abstractfs.afsrole.DISCOVER
    else:
        gateway.log_error( "Unknown role: %s" % rolestr )
        return False

    if not _initFS( driver_config, driver_secrets, role ):
        gateway.log_error( "Unable to init filesystem")
        return False

    if not fs.exists( "/" ):
        gateway.log_error( "No such file or directory: %s" % storage_dir )
        return False 

    if not fs.is_dir( "/" ):
        gateway.log_error( "Not a directory: %s" % storage_dir )
        return False

    if role == abstractfs.afsrole.DISCOVER:
        # add initial dataset
        _resync( "/" )

    return True


def driver_shutdown():
    """
    Do the one-time driver shutdown
    """
    _shutdownFS()
    

def next_dataset( driver_config, driver_secrets ):
    """
    Return the next dataset command for the AG to process.
    Should block until there's data.

    Must call gateway.crawl() to feed the data into the AG.
    Return True if there are more datasets to process.
    Return False if not.
    """

    global event_queue

    next_event = None

    # find the next file or directory 
    while True:
        next_event = None
        _lock()
        if len(event_queue) > 0:
            next_event = event_queue[0]
            event_queue.pop(0)
        _unlock()

        if next_event is None:
            # no more data at this moment.
            # sleep for 1-sec
            time.sleep( 1.0 )
            continue

        cmd = None

        if next_event.flag == ag_update_event.ENTRY_REMOVED:
            # TODO: in this case, we do not know removed one was a file or a directory
            cmd = gateway.make_metadata_command( "delete", "directory", 0555, None, next_event.path, write_ttl=DEFAULT_WRITE_TTL )
        elif next_event.flag in [ag_update_event.ENTRY_UPDATED, ag_update_event.ENTRY_ADDED]:
            if next_event.stat and not next_event.stat.directory:
                cmd = gateway.make_metadata_command( "put", "file", 0555, next_event.stat.size, next_event.path, write_ttl=DEFAULT_WRITE_TTL )
            else:
                cmd = gateway.make_metadata_command( "put", "directory", 0555, None, next_event.path, write_ttl=DEFAULT_WRITE_TTL )

        if cmd is not None:
            # send the command to the AG and get back the result
            rc = gateway.crawl( cmd )
            if rc != 0:
                gateway.log_error( "Failed to crawl %s" % cmd['path'] )

            # have more data
            return True
        else:
            # try next path 
            gateway.log_error( "unhandled changes - could not create the command" )
            continue

    return False


def refresh( request, driver_config, driver_secrets ):
    """
    Request to refresh a particular path.
    Verify that it still exists, and if so,
    queue it for republishing.
    """

    global fs

    path = gateway.request_path( request )
    file_path = gateway.path_join( "/", path )

    fs.clear_cache( file_path )
    if not fs.exists( file_path ):
        # delete
        gateway.log_error( "No longer present: '%s'" % file_path )
        entry = abstractfs.afsevent( file_path, None )
        datasets_update_cb([], [], [entry])
    else:
        # update
        gateway.log_error( "Still present: '%s'" % file_path )
        entry = abstractfs.afsevent( file_path, fs.stat( file_path ) )
        datasets_update_cb([], [], [entry])
        # resync
        _resync( os.path.dirname( file_path ) )

    return 0


def read( request, chunk_fd, driver_config, driver_secrets ):
    """
    Read a chunk of data.
    @request is a DriverRequest
    @chunk_fd is a file descriptor to which to write the data.
    @driver_config is a dict containing the driver's config
    @driver_secrets is a dict containing the driver's unencrypted secrets
    """

    global fs

    path = gateway.request_path( request )
    file_path = gateway.path_join( "/", path )
    byte_offset = gateway.request_byte_offset( request )
    byte_len = gateway.request_byte_len( request )

    if byte_offset is None:
        # this is a bug
        gateway.log_error( "BUG: byte offset of request on %s is not defined" % file_path )
        sys.exit( 1 )

    if byte_len is None:
        # this is a bug
        gateway.log_error( "BUG: byte len of request on %s is not defined" % file_path )
        sys.exit( 1 )

    if not fs.exists( file_path ):
        gateway.log_error( "No such file or directory: %s" % file_path )
        return -errno.ENOENT

    try:
        buf = fs.read( file_path, byte_offset, byte_len )
    except Exception, e:
        gateway.log_error( "Failed to read %s: %s" % ( file_path, e ) )
        return -errno.EREMOTEIO

    # send it off
    chunk_fd.write( buf )
    return 0

